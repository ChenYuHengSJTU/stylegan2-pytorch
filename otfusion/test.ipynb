{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 17:30:41.466880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 17:30:46.290983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(x):\n",
    "    return x != x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundMetric:\n",
    "    \"\"\"\n",
    "        Ground Metric object for Wasserstein computations:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, not_squared = False):\n",
    "        self.params = params\n",
    "        self.ground_metric_type = params.ground_metric\n",
    "        self.ground_metric_normalize = params.ground_metric_normalize\n",
    "        self.reg = params.reg\n",
    "        if hasattr(params, 'not_squared'):\n",
    "            self.squared = not params.not_squared\n",
    "        else:\n",
    "            # so by default squared will be on!\n",
    "            self.squared = not not_squared\n",
    "        self.mem_eff = params.ground_metric_eff\n",
    "\n",
    "    def _clip(self, ground_metric_matrix):\n",
    "        if self.params.debug:\n",
    "            print(\"before clipping\", ground_metric_matrix.data)\n",
    "\n",
    "        percent_clipped = (float((ground_metric_matrix >= self.reg * self.params.clip_max).long().sum().data) \\\n",
    "                           / ground_metric_matrix.numel()) * 100\n",
    "        print(\"percent_clipped is (assumes clip_min = 0) \", percent_clipped)\n",
    "        setattr(self.params, 'percent_clipped', percent_clipped)\n",
    "        # will keep the M' = M/reg in range clip_min and clip_max\n",
    "        ground_metric_matrix.clamp_(min=self.reg * self.params.clip_min,\n",
    "                                             max=self.reg * self.params.clip_max)\n",
    "        if self.params.debug:\n",
    "            print(\"after clipping\", ground_metric_matrix.data)\n",
    "        return ground_metric_matrix\n",
    "\n",
    "    def _normalize(self, ground_metric_matrix):\n",
    "\n",
    "        if self.ground_metric_normalize == \"log\":\n",
    "            ground_metric_matrix = torch.log1p(ground_metric_matrix)\n",
    "        elif self.ground_metric_normalize == \"max\":\n",
    "            print(\"Normalizing by max of ground metric and which is \", ground_metric_matrix.max())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.max()\n",
    "        elif self.ground_metric_normalize == \"median\":\n",
    "            print(\"Normalizing by median of ground metric and which is \", ground_metric_matrix.median())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.median()\n",
    "        elif self.ground_metric_normalize == \"mean\":\n",
    "            print(\"Normalizing by mean of ground metric and which is \", ground_metric_matrix.mean())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.mean()\n",
    "        elif self.ground_metric_normalize == \"none\":\n",
    "            return ground_metric_matrix\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return ground_metric_matrix\n",
    "\n",
    "    def _sanity_check(self, ground_metric_matrix):\n",
    "        assert not (ground_metric_matrix < 0).any()\n",
    "        assert not (isnan(ground_metric_matrix).any())\n",
    "\n",
    "    def _cost_matrix_xy(self, x, y, p=2, squared = True):\n",
    "        # TODO: Use this to guarantee reproducibility of previous results and then move onto better way\n",
    "        \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "        # (n_l,1,n_{l-1})\n",
    "        x_col = x.unsqueeze(1)\n",
    "        # (1, m_l, m_{l-1})\n",
    "        # math induction -> n_{l-1} == m_{l-1}\n",
    "        y_lin = y.unsqueeze(0)\n",
    "        # c => (n_l, m_l, m_{l-1})\n",
    "        c = torch.sum((torch.abs(x_col - y_lin)) ** p, 2)\n",
    "        if not squared:\n",
    "            print(\"dont leave off the squaring of the ground metric\")\n",
    "            c = c ** (1/2)\n",
    "        # print(c.size())\n",
    "        if self.params.dist_normalize:\n",
    "            assert NotImplementedError\n",
    "        return c\n",
    "\n",
    "\n",
    "    def _pairwise_distances(self, x, y=None, squared=True):\n",
    "        '''\n",
    "        Source: https://discuss.pytorch.org/t/efficient-distance-matrix-computation/9065/2\n",
    "        Input: x is a Nxd matrix\n",
    "               y is an optional Mxd matirx\n",
    "        Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "                if y is not given then use 'y=x'.\n",
    "        i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "        '''\n",
    "        x_norm = (x ** 2).sum(1).view(-1, 1)\n",
    "        if y is not None:\n",
    "            y_t = torch.transpose(y, 0, 1)\n",
    "            y_norm = (y ** 2).sum(1).view(1, -1)\n",
    "        else:\n",
    "            y_t = torch.transpose(x, 0, 1)\n",
    "            y_norm = x_norm.view(1, -1)\n",
    "\n",
    "        dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "        # Ensure diagonal is zero if x=y\n",
    "        dist = torch.clamp(dist, min=0.0)\n",
    "\n",
    "        if self.params.activation_histograms and self.params.dist_normalize:\n",
    "            dist = dist/self.params.act_num_samples\n",
    "            print(\"Divide squared distances by the num samples\")\n",
    "\n",
    "        if not squared:\n",
    "            print(\"dont leave off the squaring of the ground metric\")\n",
    "            dist = dist ** (1/2)\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def _get_euclidean(self, coordinates, other_coordinates=None):\n",
    "        # TODO: Replace by torch.pdist (which is said to be much more memory efficient)\n",
    "\n",
    "        if other_coordinates is None:\n",
    "            matrix = torch.norm(\n",
    "                coordinates.view(coordinates.shape[0], 1, coordinates.shape[1]) \\\n",
    "                - coordinates, p=2, dim=2\n",
    "            )\n",
    "        else:\n",
    "            # memory efficient version\n",
    "            if self.mem_eff:\n",
    "                matrix = self._pairwise_distances(coordinates, other_coordinates, squared=self.squared)\n",
    "            else:\n",
    "                matrix = self._cost_matrix_xy(coordinates, other_coordinates, squared = self.squared)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _normed_vecs(self, vecs, eps=1e-9):\n",
    "        norms = torch.norm(vecs, dim=-1, keepdim=True)\n",
    "        print(\"stats of vecs are: mean {}, min {}, max {}, std {}\".format(\n",
    "            norms.mean(), norms.min(), norms.max(), norms.std()\n",
    "        ))\n",
    "        return vecs / (norms + eps)\n",
    "\n",
    "    def _get_cosine(self, coordinates, other_coordinates=None):\n",
    "        if other_coordinates is None:\n",
    "            matrix = coordinates / torch.norm(coordinates, dim=1, keepdim=True)\n",
    "            matrix = 1 - matrix @ matrix.t()\n",
    "        else:\n",
    "            matrix = 1 - torch.div(\n",
    "                coordinates @ other_coordinates.t(),\n",
    "                torch.norm(coordinates, dim=1).view(-1, 1) @ torch.norm(other_coordinates, dim=1).view(1, -1)\n",
    "            )\n",
    "        return matrix.clamp_(min=0)\n",
    "\n",
    "    def _get_angular(self, coordinates, other_coordinates=None):\n",
    "        pass\n",
    "\n",
    "    def get_metric(self, coordinates, other_coordinates=None):\n",
    "        get_metric_map = {\n",
    "            'euclidean': self._get_euclidean,\n",
    "            'cosine': self._get_cosine,\n",
    "            'angular': self._get_angular,\n",
    "        }\n",
    "        return get_metric_map[self.ground_metric_type](coordinates, other_coordinates)\n",
    "\n",
    "    def process(self, coordinates, other_coordinates=None):\n",
    "        print('Processing the coordinates to form ground_metric')\n",
    "        if self.params.geom_ensemble_type == 'wts' and self.params.normalize_wts:\n",
    "            print(\"In weight mode: normalizing weights to unit norm\")\n",
    "            coordinates = self._normed_vecs(coordinates)\n",
    "            if other_coordinates is not None:\n",
    "                other_coordinates = self._normed_vecs(other_coordinates)\n",
    "\n",
    "        ground_metric_matrix = self.get_metric(coordinates, other_coordinates)\n",
    "\n",
    "        if self.params.debug:\n",
    "            print(\"coordinates is \", coordinates)\n",
    "            if other_coordinates is not None:\n",
    "                print(\"other_coordinates is \", other_coordinates)\n",
    "            print(\"ground_metric_matrix is \", ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        ground_metric_matrix = self._normalize(ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        if self.params.clip_gm:\n",
    "            ground_metric_matrix = self._clip(ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        if self.params.debug:\n",
    "            print(\"ground_metric_matrix at the end is \", ground_metric_matrix)\n",
    "\n",
    "        return ground_metric_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_matrix(x, y, p=2):\n",
    "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "    x_col = x.unsqueeze(1)\n",
    "    y_lin = y.unsqueeze(0)\n",
    "    c = torch.sum((torch.abs(x_col - y_lin)) ** p, 2)\n",
    "    return c\n",
    "\n",
    "def get_histogram(args, idx, cardinality, layer_name, activations=None, return_numpy = True, float64=False):\n",
    "    if activations is None:\n",
    "        # returns a uniform measure\n",
    "        if not args.unbalanced:\n",
    "            print(\"returns a uniform measure of cardinality: \", cardinality)\n",
    "            return np.ones(cardinality)/cardinality\n",
    "        else:\n",
    "            return np.ones(cardinality)\n",
    "    else:\n",
    "        # return softmax over the activations raised to a temperature\n",
    "        # layer_name is like 'fc1.weight', while activations only contains 'fc1'\n",
    "        print(activations[idx].keys())\n",
    "        unnormalized_weights = activations[idx][layer_name.split('.')[0]]\n",
    "        print(\"For layer {},  shape of unnormalized weights is \".format(layer_name), unnormalized_weights.shape)\n",
    "        unnormalized_weights = unnormalized_weights.squeeze()\n",
    "        assert unnormalized_weights.shape[0] == cardinality\n",
    "\n",
    "        if return_numpy:\n",
    "            if float64:\n",
    "                return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0).data.cpu().numpy().astype(\n",
    "                    np.float64)\n",
    "            else:\n",
    "                return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0).data.cpu().numpy()\n",
    "        else:\n",
    "            return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wassersteinized_layers_modularized(args, networks, activations=None, eps=1e-7, test_loader=None):\n",
    "    '''\n",
    "    Two neural networks that have to be averaged in geometric manner (i.e. layerwise).\n",
    "\n",
    "    The 1st network is aligned with respect to the other via wasserstein distance.\n",
    "    Also this assumes that all the layers are either fully connected or convolutional *(with no bias)*\n",
    "\n",
    "    :param networks: list of networks\n",
    "    :param activations: If not None, use it to build the activation histograms.\n",
    "    Otherwise assumes uniform distribution over neurons in a layer.\n",
    "    :return: list of layer weights 'wassersteinized'\n",
    "    '''\n",
    "\n",
    "    # simple_model_0, simple_model_1 = networks[0], networks[1]\n",
    "    # simple_model_0 = get_trained_model(0, model='simplenet')\n",
    "    # simple_model_1 = get_trained_model(1, model='simplenet')\n",
    "\n",
    "    avg_aligned_layers = []\n",
    "    # cumulative_T_var = None\n",
    "    T_var = None\n",
    "    # print(list(networks[0].parameters()))\n",
    "    previous_layer_shape = None\n",
    "    ground_metric_object = GroundMetric(args)\n",
    "\n",
    "    if args.eval_aligned:\n",
    "        model0_aligned_layers = []\n",
    "\n",
    "    if args.gpu_id==-1:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "\n",
    "    num_layers = len(list(zip(networks[0].parameters(), networks[1].parameters())))\n",
    "    # named_parameters is an generator\n",
    "    # for linear layer with bias, the affine matrix and bias will be two separate terms in named_parameters\n",
    "    for idx, ((layer0_name, fc_layer0_weight), (layer1_name, fc_layer1_weight)) in \\\n",
    "            enumerate(zip(networks[0].named_parameters(), networks[1].named_parameters())):\n",
    "\n",
    "        assert fc_layer0_weight.shape == fc_layer1_weight.shape\n",
    "        print(\"Previous layer shape is \", previous_layer_shape)\n",
    "        previous_layer_shape = fc_layer1_weight.shape\n",
    "\n",
    "        mu_cardinality = fc_layer0_weight.shape[0]\n",
    "        nu_cardinality = fc_layer1_weight.shape[0]\n",
    "\n",
    "        # mu = np.ones(fc_layer0_weight.shape[0])/fc_layer0_weight.shape[0]\n",
    "        # nu = np.ones(fc_layer1_weight.shape[0])/fc_layer1_weight.shape[0]\n",
    "\n",
    "        layer_shape = fc_layer0_weight.shape\n",
    "        if len(layer_shape) > 2:\n",
    "            is_conv = True\n",
    "            # For convolutional layers, it is (#out_channels, #in_channels, height, width)\n",
    "            fc_layer0_weight_data = fc_layer0_weight.data.view(fc_layer0_weight.shape[0], fc_layer0_weight.shape[1], -1)\n",
    "            fc_layer1_weight_data = fc_layer1_weight.data.view(fc_layer1_weight.shape[0], fc_layer1_weight.shape[1], -1)\n",
    "        else:\n",
    "            is_conv = False\n",
    "            fc_layer0_weight_data = fc_layer0_weight.data\n",
    "            fc_layer1_weight_data = fc_layer1_weight.data\n",
    "\n",
    "        if idx == 0:\n",
    "            if is_conv:\n",
    "                M = ground_metric_object.process(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n",
    "                                fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "                # M = cost_matrix(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n",
    "                #                 fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "            else:\n",
    "                # print(\"layer data is \", fc_layer0_weight_data, fc_layer1_weight_data)\n",
    "                M = ground_metric_object.process(fc_layer0_weight_data, fc_layer1_weight_data)\n",
    "                # M = cost_matrix(fc_layer0_weight, fc_layer1_weight)\n",
    "\n",
    "            aligned_wt = fc_layer0_weight_data\n",
    "        else:\n",
    "\n",
    "            print(\"shape of layer: model 0\", fc_layer0_weight_data.shape)\n",
    "            print(\"shape of layer: model 1\", fc_layer1_weight_data.shape)\n",
    "            print(\"shape of previous transport map\", T_var.shape)\n",
    "\n",
    "            # aligned_wt = None, this caches the tensor and causes OOM\n",
    "            if is_conv:\n",
    "                T_var_conv = T_var.unsqueeze(0).repeat(fc_layer0_weight_data.shape[2], 1, 1)\n",
    "                aligned_wt = torch.bmm(fc_layer0_weight_data.permute(2, 0, 1), T_var_conv).permute(1, 2, 0)\n",
    "\n",
    "                M = ground_metric_object.process(\n",
    "                    aligned_wt.contiguous().view(aligned_wt.shape[0], -1),\n",
    "                    fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1)\n",
    "                )\n",
    "            else:\n",
    "                # ? why \n",
    "                if fc_layer0_weight.data.shape[1] != T_var.shape[0]:\n",
    "                    # Handles the switch from convolutional layers to fc layers\n",
    "                    fc_layer0_unflattened = fc_layer0_weight.data.view(fc_layer0_weight.shape[0], T_var.shape[0], -1).permute(2, 0, 1)\n",
    "                    aligned_wt = torch.bmm(\n",
    "                        fc_layer0_unflattened,\n",
    "                        T_var.unsqueeze(0).repeat(fc_layer0_unflattened.shape[0], 1, 1)\n",
    "                    ).permute(1, 2, 0)\n",
    "                    aligned_wt = aligned_wt.contiguous().view(aligned_wt.shape[0], -1)\n",
    "                else:\n",
    "                    # print(\"layer data (aligned) is \", aligned_wt, fc_layer1_weight_data)\n",
    "                    aligned_wt = torch.matmul(fc_layer0_weight.data, T_var)\n",
    "                # M = cost_matrix(aligned_wt, fc_layer1_weight)\n",
    "                M = ground_metric_object.process(aligned_wt, fc_layer1_weight)\n",
    "            #     print(\"ground metric is \", M)\n",
    "            # if args.skip_last_layer and idx == (num_layers - 1):\n",
    "            #     print(\"Simple averaging of last layer weights. NO transport map needs to be computed\")\n",
    "            #     if args.ensemble_step != 0.5:\n",
    "            #         avg_aligned_layers.append((1 - args.ensemble_step) * aligned_wt +\n",
    "            #                               args.ensemble_step * fc_layer1_weight)\n",
    "            #     else:\n",
    "            #         avg_aligned_layers.append((aligned_wt + fc_layer1_weight)/2)\n",
    "            #     return avg_aligned_layers\n",
    "\n",
    "        if args.importance is None or (idx == num_layers -1):\n",
    "            mu = get_histogram(args, 0, mu_cardinality, layer0_name)\n",
    "            nu = get_histogram(args, 1, nu_cardinality, layer1_name)\n",
    "        # else:\n",
    "        #     # mu = _get_neuron_importance_histogram(args, aligned_wt, is_conv)\n",
    "        #     mu = _get_neuron_importance_histogram(args, fc_layer0_weight_data, is_conv)\n",
    "        #     nu = _get_neuron_importance_histogram(args, fc_layer1_weight_data, is_conv)\n",
    "        #     print(mu, nu)\n",
    "        #     assert args.proper_marginals\n",
    "\n",
    "        cpuM = M.data.cpu().numpy()\n",
    "        if args.exact:\n",
    "            T = ot.emd(mu, nu, cpuM)\n",
    "        else:\n",
    "            T = ot.bregman.sinkhorn(mu, nu, cpuM, reg=args.reg)\n",
    "        # T = ot.emd(mu, nu, log_cpuM)\n",
    "\n",
    "        if args.gpu_id!=-1:\n",
    "            T_var = torch.from_numpy(T).cuda(args.gpu_id).float()\n",
    "        else:\n",
    "            T_var = torch.from_numpy(T).float()\n",
    "\n",
    "        # torch.set_printoptions(profile=\"full\")\n",
    "        print(\"the transport map is \", T_var)\n",
    "        # torch.set_printoptions(profile=\"default\")\n",
    "\n",
    "        if args.correction:\n",
    "            if not args.proper_marginals:\n",
    "                # think of it as m x 1, scaling weights for m linear combinations of points in X\n",
    "                if args.gpu_id != -1:\n",
    "                    # marginals = torch.mv(T_var.t(), torch.ones(T_var.shape[0]).cuda(args.gpu_id))  # T.t().shape[1] = T.shape[0]\n",
    "                    marginals = torch.ones(T_var.shape[0]).cuda(args.gpu_id) / T_var.shape[0]\n",
    "                else:\n",
    "                    # marginals = torch.mv(T_var.t(),\n",
    "                    #                      torch.ones(T_var.shape[0]))  # T.t().shape[1] = T.shape[0]\n",
    "                    marginals = torch.ones(T_var.shape[0]) / T_var.shape[0]\n",
    "                marginals = torch.diag(1.0/(marginals + eps))  # take inverse\n",
    "                T_var = torch.matmul(T_var, marginals)\n",
    "            else:\n",
    "                # marginals_alpha = T_var @ torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)\n",
    "                marginals_beta = T_var.t() @ torch.ones(T_var.shape[0], dtype=T_var.dtype).to(device)\n",
    "\n",
    "                marginals = (1 / (marginals_beta + eps))\n",
    "                print(\"shape of inverse marginals beta is \", marginals_beta.shape)\n",
    "                print(\"inverse marginals beta is \", marginals_beta)\n",
    "\n",
    "                T_var = T_var * marginals\n",
    "                # i.e., how a neuron of 2nd model is constituted by the neurons of 1st model\n",
    "                # this should all be ones, and number equal to number of neurons in 2nd model\n",
    "                print(T_var.sum(dim=0))\n",
    "                # assert (T_var.sum(dim=0) == torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)).all()\n",
    "\n",
    "        if args.debug:\n",
    "            if idx == (num_layers - 1):\n",
    "                print(\"there goes the last transport map: \\n \", T_var)\n",
    "            else:\n",
    "                print(\"there goes the transport map at layer {}: \\n \".format(idx), T_var)\n",
    "\n",
    "            print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n",
    "\n",
    "        print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n",
    "        print(\"Here, trace is {} and matrix sum is {} \".format(torch.trace(T_var), torch.sum(T_var)))\n",
    "        setattr(args, 'trace_sum_ratio_{}'.format(layer0_name), (torch.trace(T_var) / torch.sum(T_var)).item())\n",
    "\n",
    "        if args.past_correction:\n",
    "            print(\"this is past correction for weight mode\")\n",
    "            print(\"Shape of aligned wt is \", aligned_wt.shape)\n",
    "            print(\"Shape of fc_layer0_weight_data is \", fc_layer0_weight_data.shape)\n",
    "            t_fc0_model = torch.matmul(T_var.t(), aligned_wt.contiguous().view(aligned_wt.shape[0], -1))\n",
    "        else:\n",
    "            t_fc0_model = torch.matmul(T_var.t(), fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1))\n",
    "\n",
    "        # Average the weights of aligned first layers\n",
    "        if args.ensemble_step != 0.5:\n",
    "            geometric_fc = ((1-args.ensemble_step) * t_fc0_model +\n",
    "                            args.ensemble_step * fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "        else:\n",
    "            geometric_fc = (t_fc0_model + fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))/2\n",
    "        if is_conv and layer_shape != geometric_fc.shape:\n",
    "            geometric_fc = geometric_fc.view(layer_shape)\n",
    "        avg_aligned_layers.append(geometric_fc)\n",
    "\n",
    "        # # get the performance of the model 0 aligned with respect to the model 1\n",
    "        # if args.eval_aligned:\n",
    "        #     if is_conv and layer_shape != t_fc0_model.shape:\n",
    "        #         t_fc0_model = t_fc0_model.view(layer_shape)\n",
    "        #     model0_aligned_layers.append(t_fc0_model)\n",
    "        #     _, acc = update_model(args, networks[0], model0_aligned_layers, test=True,\n",
    "        #                           test_loader=test_loader, idx=0)\n",
    "        #     print(\"For layer idx {}, accuracy of the updated model is {}\".format(idx, acc))\n",
    "        #     setattr(args, 'model0_aligned_acc_layer_{}'.format(str(idx)), acc)\n",
    "        #     if idx == (num_layers - 1):\n",
    "        #         setattr(args, 'model0_aligned_acc', acc)\n",
    "\n",
    "    return avg_aligned_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        # print(\"After encoder: \", result.shape)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(\"After encoder and flatten: \", result.shape)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        # print(\"mu:\", mu.shape)\n",
    "        # print(\"log_var;\", log_var.shape)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        # print(\"after decoder_input layer:\", result.shape)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        # print(\"after reshape:\", result.shape)\n",
    "        result = self.decoder(result)\n",
    "        # print(\"after decoder:\", result.shape)\n",
    "        result = self.final_layer(result)\n",
    "        # print(\"after final layer:\", result.shape)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = VanillaVAE(3, 128)\n",
    "model2 = VanillaVAE(3, 128)\n",
    "dict1 = dict(torch.load('vae_nob_bias_adagrad_1.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "dict2 = dict(torch.load('vae_nob_bias_adagrad_2.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "tmp1 = dict()\n",
    "tmp2 = dict()\n",
    "\n",
    "# print(dict1.keys())\n",
    "for k in dict1.keys():\n",
    "    v1 = dict1[k]\n",
    "    v2 = dict2[k]\n",
    "    k = k[6:]\n",
    "    # print(k)\n",
    "    # if \"running\" in k:\n",
    "    #     print(v1 - v2)\n",
    "    tmp1[k] = v1\n",
    "    tmp2[k] = v2\n",
    "\n",
    "# for k in dict2.keys():\n",
    "#     v = dict2[k]\n",
    "#     k = k[6:]\n",
    "#     # print(k)\n",
    "#     tmp2[k] = v\n",
    "\n",
    "model1.load_state_dict(tmp1)\n",
    "model2.load_state_dict(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VanillaVAE(\n",
       "   (encoder): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (3): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (4): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "   )\n",
       "   (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
       "   (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
       "   (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
       "   (decoder): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (2): Sequential(\n",
       "       (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (3): Sequential(\n",
       "       (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "   )\n",
       "   (final_layer): Sequential(\n",
       "     (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.01)\n",
       "     (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (4): Tanh()\n",
       "   )\n",
       " ),\n",
       " VanillaVAE(\n",
       "   (encoder): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (3): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (4): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "   )\n",
       "   (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
       "   (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
       "   (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
       "   (decoder): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (2): Sequential(\n",
       "       (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (3): Sequential(\n",
       "       (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "   )\n",
       "   (final_layer): Sequential(\n",
       "     (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.01)\n",
       "     (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (4): Tanh()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vgg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mparameters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_parameters\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m get_parameters()\n\u001b[1;32m      5\u001b[0m get_wassersteinized_layers_modularized(args, [model1, model2])\n",
      "File \u001b[0;32m/mnt/d/SJTU/phd/Code/otfusion/parameters.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dotdict, get_timestamp_other, mkdir\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/SJTU/phd/Code/otfusion/utils.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m PATH_TO_VGG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cifar/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(PATH_TO_VGG)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvgg\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpartition\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_timestamp_other\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vgg'"
     ]
    }
   ],
   "source": [
    "from parameters import get_parameters\n",
    "\n",
    "args = get_parameters()\n",
    "\n",
    "get_wassersteinized_layers_modularized(args, [model1, model2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import CelebA\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.CenterCrop(148),\n",
    "                                        transforms.Resize(64),\n",
    "                                        transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CelebA(root='/home/chenyuheng/celeba', split='test', download=False, transform=train_transforms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:33<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020665558407274194, 'recon_loss': 0.014973822381132509, 'kl_loss': -22.76694299013185}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:29<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020688911350682758, 'recon_loss': 0.014985132356508623, 'kl_loss': -22.815114779350075}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:29<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020706728124656758, 'recon_loss': 0.01498941203746467, 'kl_loss': -22.869263147696476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:30<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0207064473309005, 'recon_loss': 0.014978268395703384, 'kl_loss': -22.912714597506397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:29<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.02071963801072579, 'recon_loss': 0.014977033094813423, 'kl_loss': -22.970418532689436}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:30<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.02069195717549287, 'recon_loss': 0.014935361669183916, 'kl_loss': -23.02638082626542}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:32<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020625482689445977, 'recon_loss': 0.014851278795574147, 'kl_loss': -23.096814357317413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:31<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020577109753130325, 'recon_loss': 0.014786915516313651, 'kl_loss': -23.16077612607907}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:29<00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.020502921534129057, 'recon_loss': 0.014692730876282813, 'kl_loss': -23.24076166519752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loaddata():\n",
    "    return torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)   \n",
    "\n",
    "from tqdm import tqdm\n",
    "def test(model):\n",
    "    model.cuda()\n",
    "    dataloader = loaddata()\n",
    "    i = 0\n",
    "    loss = 0.0\n",
    "    recon_loss = 0.0\n",
    "    kl_loss = 0.0\n",
    "    for img, _ in tqdm(dataloader):\n",
    "        # print(img)\n",
    "        out = model(img.cuda())\n",
    "        # loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (i + 1)\n",
    "        loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (i + 1)\n",
    "        recon_loss = recon_loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['Reconstruction_Loss'].item() / (i + 1)\n",
    "        kl_loss = kl_loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['KLD'].item() / (i + 1)\n",
    "        # loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (64 * (i + 1))\n",
    "        i += 1\n",
    "    return {'loss': loss, 'recon_loss': recon_loss, 'kl_loss': kl_loss}\n",
    "\n",
    "def get_avg_model(t):\n",
    "    avg_model = VanillaVAE(3, 128)\n",
    "    avg_tmp = dict()\n",
    "    for k in dict1.keys():\n",
    "        v1 = dict1[k]\n",
    "        v2 = dict2[k]\n",
    "        k = k[6:]\n",
    "        avg_tmp[k] = t * v1 + (1 - t) * v2\n",
    "    avg_model.load_state_dict(avg_tmp)\n",
    "    return avg_model\n",
    "\n",
    "for t in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    print(test(get_avg_model(t)))\n",
    "# print(test(fused_model))\n",
    "print(test(model1), test(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:30<00:00, 10.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.020466407510237063,\n",
       " 'recon_loss': 0.014629446235724179,\n",
       " 'kl_loss': -23.34784392821486}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:30<00:00, 10.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.02069165680008249,\n",
       " 'recon_loss': 0.015014265783322168,\n",
       " 'kl_loss': -22.709562717339914}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Tanh()\n",
      "  )\n",
      ") VanillaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3]) torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3]) torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3]) torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3]) torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([512, 256, 3, 3]) torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([128, 2048]) torch.Size([128, 2048])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([128, 2048]) torch.Size([128, 2048])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([2048, 128]) torch.Size([2048, 128])\n",
      "torch.Size([2048]) torch.Size([2048])\n",
      "torch.Size([512, 256, 3, 3]) torch.Size([512, 256, 3, 3])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256, 128, 3, 3]) torch.Size([256, 128, 3, 3])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([128, 64, 3, 3]) torch.Size([128, 64, 3, 3])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64, 32, 3, 3]) torch.Size([64, 32, 3, 3])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([32, 32, 3, 3]) torch.Size([32, 32, 3, 3])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([3, 32, 3, 3]) torch.Size([3, 32, 3, 3])\n",
      "torch.Size([3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "param_diff = 0.0\n",
    "for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "    print(param1.shape, param2.shape)\n",
    "    param_diff += torch.square(param1.cpu() - param2.cpu()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(253.8647, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = VanillaVAE(3, 128)\n",
    "model4 = VanillaVAE(3, 128)\n",
    "\n",
    "dict3 = dict(torch.load('last7.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "dict4 = dict(torch.load('last8.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "tmp3 = dict()\n",
    "tmp4 = dict()\n",
    "\n",
    "# print(dict1.keys())\n",
    "for k in dict3.keys():\n",
    "    v1 = dict3[k]\n",
    "    v2 = dict4[k]\n",
    "    k = k[6:]\n",
    "    # print(k)\n",
    "    # if \"running\" in k:\n",
    "    #     print(v1 - v2)\n",
    "    tmp3[k] = v1\n",
    "    tmp4[k] = v2\n",
    "\n",
    "# for k in dict2.keys():\n",
    "#     v = dict2[k]\n",
    "#     k = k[6:]\n",
    "#     # print(k)\n",
    "#     tmp2[k] = v\n",
    "\n",
    "model3.load_state_dict(tmp3)\n",
    "model4.load_state_dict(tmp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1775210., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_diff = 0.0\n",
    "for param1, param2 in zip(model3.parameters(), model4.parameters()):\n",
    "    # print(param1.shape, param2.shape)\n",
    "    param_diff += torch.square(param1.cpu() - param2.cpu()).sum()\n",
    "param_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "lst1 = [1, 2, 3]\n",
    "lst2 = [4, 5, 6]\n",
    "for i,j in zip(lst1, lst2):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "# import PIL.Image\n",
    "img = PIL.Image.open(\"origin_VanillaVAE_Epoch_0.png\")\n",
    "train_transforms(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoder:  torch.Size([1, 512, 2, 2])\n",
      "After encoder and flatten:  torch.Size([1, 2048])\n",
      "mu: torch.Size([1, 128])\n",
      "log_var; torch.Size([1, 128])\n",
      "after decoder_input layer: torch.Size([1, 2048])\n",
      "after reshape: torch.Size([1, 512, 2, 2])\n",
      "after decoder: torch.Size([1, 32, 32, 32])\n",
      "after final layer: torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.2730,  0.2414,  0.3393,  ...,  0.0675,  0.0332,  0.0380],\n",
       "           [ 0.2811,  0.2200,  0.2551,  ..., -0.0875, -0.1410, -0.0063],\n",
       "           [ 0.3443,  0.2530,  0.3116,  ..., -0.1292, -0.1672,  0.0112],\n",
       "           ...,\n",
       "           [ 0.4955,  0.3060,  0.2658,  ..., -0.0529, -0.1468, -0.0974],\n",
       "           [ 0.4762,  0.1809,  0.2290,  ..., -0.1909, -0.1936, -0.0057],\n",
       "           [ 0.5777,  0.3117,  0.3108,  ..., -0.0562, -0.0455,  0.0692]],\n",
       " \n",
       "          [[ 0.3769,  0.3818,  0.4219,  ...,  0.1275,  0.0876,  0.0785],\n",
       "           [ 0.4299,  0.4136,  0.4111,  ...,  0.0797,  0.0208,  0.0655],\n",
       "           [ 0.4577,  0.4050,  0.4144,  ...,  0.0268, -0.0462,  0.0297],\n",
       "           ...,\n",
       "           [ 0.6456,  0.5882,  0.5461,  ...,  0.1249,  0.0311, -0.0054],\n",
       "           [ 0.6297,  0.4919,  0.4908,  ...,  0.0103,  0.0086,  0.0778],\n",
       "           [ 0.6626,  0.5236,  0.4855,  ...,  0.1008,  0.0734,  0.0894]],\n",
       " \n",
       "          [[ 0.4331,  0.4576,  0.5088,  ...,  0.1222,  0.1477,  0.1713],\n",
       "           [ 0.4589,  0.3799,  0.4108,  ...,  0.1298,  0.1824,  0.2119],\n",
       "           [ 0.4523,  0.3726,  0.4129,  ...,  0.1271,  0.1677,  0.1911],\n",
       "           ...,\n",
       "           [ 0.7387,  0.7272,  0.7570,  ...,  0.1646,  0.2707,  0.2547],\n",
       "           [ 0.7221,  0.6902,  0.7234,  ...,  0.1134,  0.2672,  0.3249],\n",
       "           [ 0.7443,  0.7036,  0.6777,  ...,  0.1816,  0.2100,  0.2219]]]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " tensor([[[[0.1412, 0.0157, 0.0157,  ..., 0.8863, 1.0000, 0.9961],\n",
       "           [0.1725, 0.0157, 0.0157,  ..., 0.8863, 0.9961, 0.9804],\n",
       "           [0.1843, 0.0196, 0.0157,  ..., 0.7843, 0.8588, 0.6745],\n",
       "           ...,\n",
       "           [0.1255, 0.1255, 0.1176,  ..., 0.5922, 0.6471, 0.6235],\n",
       "           [0.1882, 0.1882, 0.1686,  ..., 0.6667, 0.7059, 0.6863],\n",
       "           [0.2000, 0.2000, 0.1765,  ..., 0.6471, 0.6980, 0.6824]],\n",
       " \n",
       "          [[0.2157, 0.0510, 0.0392,  ..., 0.8863, 1.0000, 0.9961],\n",
       "           [0.2627, 0.0627, 0.0353,  ..., 0.8863, 0.9961, 0.9804],\n",
       "           [0.2706, 0.0588, 0.0314,  ..., 0.7843, 0.8588, 0.6745],\n",
       "           ...,\n",
       "           [0.1176, 0.1216, 0.1098,  ..., 0.4706, 0.5059, 0.4627],\n",
       "           [0.1804, 0.1804, 0.1608,  ..., 0.5333, 0.5255, 0.4941],\n",
       "           [0.1922, 0.1922, 0.1725,  ..., 0.5020, 0.4941, 0.4824]],\n",
       " \n",
       "          [[0.2941, 0.0706, 0.0510,  ..., 0.8863, 1.0000, 0.9961],\n",
       "           [0.3412, 0.0824, 0.0510,  ..., 0.8863, 1.0000, 0.9843],\n",
       "           [0.3569, 0.0863, 0.0510,  ..., 0.7882, 0.8627, 0.6824],\n",
       "           ...,\n",
       "           [0.1216, 0.1255, 0.1137,  ..., 0.3529, 0.3725, 0.3333],\n",
       "           [0.1843, 0.1843, 0.1647,  ..., 0.3961, 0.3804, 0.3569],\n",
       "           [0.2000, 0.2000, 0.1765,  ..., 0.3686, 0.3569, 0.3373]]]]),\n",
       " tensor([[-1.8307e-01, -8.9406e-02,  3.9006e-01,  1.5928e-02,  2.2022e-01,\n",
       "          -2.0924e-01,  6.8714e-02, -1.2878e-01, -1.3614e-01, -1.3979e-01,\n",
       "          -1.1077e+00, -7.6544e-02,  7.2279e-02, -1.0663e-01, -3.0190e-01,\n",
       "           2.6708e-02, -6.2036e-02, -9.3413e-02, -2.9610e-01, -8.1201e-02,\n",
       "           2.0325e-01,  2.4476e-01,  2.9101e-03, -1.7853e-02,  1.2667e-03,\n",
       "          -8.8870e-02,  2.2077e-01, -1.1303e-02,  2.3749e+00, -6.9961e-02,\n",
       "          -1.7175e-01, -1.7660e-01, -3.7976e-02, -1.9741e-01, -1.2857e-01,\n",
       "           9.0631e-02, -2.5203e-02, -3.4834e-02, -1.8920e-01, -3.4628e+00,\n",
       "          -6.8369e-02, -3.0483e-01,  8.5293e-02,  1.4760e-01, -4.0041e-01,\n",
       "           1.5463e+00, -2.4598e-02, -3.3512e-02,  4.2226e-01,  6.5143e-02,\n",
       "          -3.9406e-03, -9.2103e-01,  1.2253e-01,  5.7333e-01,  2.3487e-01,\n",
       "           3.8742e-02, -6.5527e-02, -6.8604e-02,  5.1252e-02,  9.8430e-02,\n",
       "           4.0112e-02, -5.1650e-01, -1.4441e-01,  5.9956e-02,  3.1500e-02,\n",
       "           8.2946e-01, -4.1193e-01, -2.3893e-02,  2.7861e-01, -1.9348e-01,\n",
       "           3.2086e-01,  1.3259e-01, -6.5625e-01, -7.5904e-01,  1.4455e-01,\n",
       "          -1.5945e-01,  1.4339e-01,  8.8791e-01, -1.2335e-02,  1.4296e-01,\n",
       "           6.3211e-01,  1.3822e-01,  2.1292e-01, -2.5291e-02,  1.3811e-01,\n",
       "          -2.7498e-01,  1.0731e-01, -1.6903e-01, -1.1518e-03,  2.1688e-01,\n",
       "           2.8824e-03, -1.4263e-01,  6.6939e-01, -5.1842e-02,  1.3996e-01,\n",
       "           1.0089e-01,  4.3052e-02, -1.0913e-01,  6.0957e-01, -2.8543e-02,\n",
       "           8.0075e-02,  1.2461e-01, -1.3430e-01,  1.8886e-01, -1.1985e-01,\n",
       "           1.3118e-01, -1.0325e+00, -1.5007e-02,  1.6109e-01,  4.6951e-01,\n",
       "           7.9221e-02,  2.4340e-01,  9.4394e-02, -1.7736e-01, -7.6791e-02,\n",
       "           5.5775e-02,  9.1136e-02,  5.7471e-02, -1.1399e-02,  2.1728e-01,\n",
       "           4.9305e-01,  1.7243e+00, -7.7037e-02, -2.7747e-02, -5.9739e-02,\n",
       "           4.9621e-02,  9.9450e-02, -3.6309e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 9.5738e-04,  9.6636e-03, -4.3907e-02, -6.7318e-03,  7.4219e-04,\n",
       "           3.3679e-03,  8.6995e-03,  5.6730e-03, -1.1332e-02, -4.7209e-02,\n",
       "          -1.2293e+00, -1.1723e-03, -9.9220e-03, -5.0215e-03, -3.5077e-01,\n",
       "          -6.9814e-04, -2.6235e-02,  2.5065e-02, -1.3316e+00,  1.5698e-02,\n",
       "          -2.3950e-02, -2.0104e-02, -1.5856e-02,  1.4294e-02,  6.5240e-03,\n",
       "          -2.6279e-03, -2.3682e-03, -2.1351e-03, -1.1978e+00,  1.0237e-02,\n",
       "          -3.4216e-03, -4.6042e-03, -2.1394e-02, -2.1473e-02, -2.8285e-04,\n",
       "           9.0797e-03, -2.0979e-02, -5.6389e-04, -2.0844e-02, -2.4499e-01,\n",
       "           2.1257e-02, -2.6996e-01, -1.7341e-02,  3.4485e-02, -2.2876e+00,\n",
       "          -2.3679e-01, -1.1376e-02, -2.9812e-03, -3.1781e-02,  1.3269e-02,\n",
       "           1.3318e-02,  9.4074e-02, -1.2703e-02, -8.2296e-01,  5.4648e-03,\n",
       "          -2.8264e-03, -9.5088e-03,  4.8707e-03,  1.6168e-03, -5.1834e-01,\n",
       "           9.5434e-03, -4.5383e-01,  2.1828e-03, -8.8554e-04, -1.5606e-03,\n",
       "          -9.5535e-02, -3.3283e-02,  2.7585e-02, -9.0174e-03, -1.0903e-02,\n",
       "          -2.6382e-01, -3.4593e-02, -2.7473e+00, -2.2169e+00, -1.5458e-02,\n",
       "          -8.1419e-01, -1.3239e-02, -1.0722e+00, -4.6307e-02,  3.1921e-03,\n",
       "          -1.9569e+00, -4.5782e-04, -6.0697e-03, -1.2058e-02,  5.0618e-03,\n",
       "          -9.1370e-03,  1.7720e-02,  6.5090e-03,  1.0648e-02, -5.7933e-03,\n",
       "          -1.0874e-02, -1.4790e-02, -9.5304e-01, -1.1215e-02, -2.2015e-02,\n",
       "          -3.1016e-02,  1.7250e-02,  8.5781e-03, -7.2485e-01, -1.7004e-02,\n",
       "           5.5024e-03, -4.1871e-01, -6.2721e-01, -2.6681e-02, -3.3140e-03,\n",
       "           3.3667e-02, -2.4671e+00,  9.1702e-03,  1.3165e-02,  2.5226e-02,\n",
       "           1.7077e-02,  6.3899e-04, -6.3810e-03,  2.2585e-02,  2.0392e-02,\n",
       "          -3.9554e+00,  1.4432e-02, -2.7053e-03,  2.0868e-02, -1.7693e-02,\n",
       "          -2.4380e+00, -2.7120e+00,  4.3663e-03, -1.9915e-02,  1.5140e-02,\n",
       "          -8.1940e-03, -7.4046e-01, -3.2317e-02]], grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(train_transforms(img).unsqueeze(0))\n",
    "# model1(train_transforms(img).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor(1, 3, 64, 64)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize VanillaVAE track_running_stats:False bias:True\n",
      "graph():\n",
      "    %input_1 : ~torch.tensor [#users=2] = placeholder[target=input]\n",
      "    %_kwargs : [#users=0] = placeholder[target=**kwargs]\n",
      "    %encoder_0_0 : [#users=1] = call_module[target=encoder.0.0](args = (%input_1,), kwargs = {})\n",
      "    %encoder_0_1 : [#users=1] = call_module[target=encoder.0.1](args = (%encoder_0_0,), kwargs = {})\n",
      "    %encoder_0_2 : [#users=1] = call_module[target=encoder.0.2](args = (%encoder_0_1,), kwargs = {})\n",
      "    %encoder_1_0 : [#users=1] = call_module[target=encoder.1.0](args = (%encoder_0_2,), kwargs = {})\n",
      "    %encoder_1_1 : [#users=1] = call_module[target=encoder.1.1](args = (%encoder_1_0,), kwargs = {})\n",
      "    %encoder_1_2 : [#users=1] = call_module[target=encoder.1.2](args = (%encoder_1_1,), kwargs = {})\n",
      "    %encoder_2_0 : [#users=1] = call_module[target=encoder.2.0](args = (%encoder_1_2,), kwargs = {})\n",
      "    %encoder_2_1 : [#users=1] = call_module[target=encoder.2.1](args = (%encoder_2_0,), kwargs = {})\n",
      "    %encoder_2_2 : [#users=1] = call_module[target=encoder.2.2](args = (%encoder_2_1,), kwargs = {})\n",
      "    %encoder_3_0 : [#users=1] = call_module[target=encoder.3.0](args = (%encoder_2_2,), kwargs = {})\n",
      "    %encoder_3_1 : [#users=1] = call_module[target=encoder.3.1](args = (%encoder_3_0,), kwargs = {})\n",
      "    %encoder_3_2 : [#users=1] = call_module[target=encoder.3.2](args = (%encoder_3_1,), kwargs = {})\n",
      "    %encoder_4_0 : [#users=1] = call_module[target=encoder.4.0](args = (%encoder_3_2,), kwargs = {})\n",
      "    %encoder_4_1 : [#users=1] = call_module[target=encoder.4.1](args = (%encoder_4_0,), kwargs = {})\n",
      "    %encoder_4_2 : [#users=1] = call_module[target=encoder.4.2](args = (%encoder_4_1,), kwargs = {})\n",
      "    %encoder_5_0 : [#users=1] = call_module[target=encoder.5.0](args = (%encoder_4_2,), kwargs = {})\n",
      "    %encoder_5_1 : [#users=1] = call_module[target=encoder.5.1](args = (%encoder_5_0,), kwargs = {})\n",
      "    %encoder_5_2 : [#users=1] = call_module[target=encoder.5.2](args = (%encoder_5_1,), kwargs = {})\n",
      "    %flatten : [#users=2] = call_function[target=torch.flatten](args = (%encoder_5_2,), kwargs = {start_dim: 1})\n",
      "    %fc_mu : [#users=2] = call_module[target=fc_mu](args = (%flatten,), kwargs = {})\n",
      "    %fc_var : [#users=2] = call_module[target=fc_var](args = (%flatten,), kwargs = {})\n",
      "    %mul : [#users=1] = call_function[target=operator.mul](args = (0.5, %fc_var), kwargs = {})\n",
      "    %exp : [#users=2] = call_function[target=torch.exp](args = (%mul,), kwargs = {})\n",
      "    %randn_like : [#users=1] = call_function[target=torch.randn_like](args = (%exp,), kwargs = {})\n",
      "    %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%randn_like, %exp), kwargs = {})\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%mul_1, %fc_mu), kwargs = {})\n",
      "    %decoder_input : [#users=1] = call_module[target=decoder_input](args = (%add,), kwargs = {})\n",
      "    %view : [#users=1] = call_method[target=view](args = (%decoder_input, -1, 1024, 1, 1), kwargs = {})\n",
      "    %decoder_0_0 : [#users=1] = call_module[target=decoder.0.0](args = (%view,), kwargs = {})\n",
      "    %decoder_0_1 : [#users=1] = call_module[target=decoder.0.1](args = (%decoder_0_0,), kwargs = {})\n",
      "    %decoder_0_2 : [#users=1] = call_module[target=decoder.0.2](args = (%decoder_0_1,), kwargs = {})\n",
      "    %decoder_1_0 : [#users=1] = call_module[target=decoder.1.0](args = (%decoder_0_2,), kwargs = {})\n",
      "    %decoder_1_1 : [#users=1] = call_module[target=decoder.1.1](args = (%decoder_1_0,), kwargs = {})\n",
      "    %decoder_1_2 : [#users=1] = call_module[target=decoder.1.2](args = (%decoder_1_1,), kwargs = {})\n",
      "    %decoder_2_0 : [#users=1] = call_module[target=decoder.2.0](args = (%decoder_1_2,), kwargs = {})\n",
      "    %decoder_2_1 : [#users=1] = call_module[target=decoder.2.1](args = (%decoder_2_0,), kwargs = {})\n",
      "    %decoder_2_2 : [#users=1] = call_module[target=decoder.2.2](args = (%decoder_2_1,), kwargs = {})\n",
      "    %decoder_3_0 : [#users=1] = call_module[target=decoder.3.0](args = (%decoder_2_2,), kwargs = {})\n",
      "    %decoder_3_1 : [#users=1] = call_module[target=decoder.3.1](args = (%decoder_3_0,), kwargs = {})\n",
      "    %decoder_3_2 : [#users=1] = call_module[target=decoder.3.2](args = (%decoder_3_1,), kwargs = {})\n",
      "    %decoder_4_0 : [#users=1] = call_module[target=decoder.4.0](args = (%decoder_3_2,), kwargs = {})\n",
      "    %decoder_4_1 : [#users=1] = call_module[target=decoder.4.1](args = (%decoder_4_0,), kwargs = {})\n",
      "    %decoder_4_2 : [#users=1] = call_module[target=decoder.4.2](args = (%decoder_4_1,), kwargs = {})\n",
      "    %final_layer_0 : [#users=1] = call_module[target=final_layer.0](args = (%decoder_4_2,), kwargs = {})\n",
      "    %final_layer_1 : [#users=1] = call_module[target=final_layer.1](args = (%final_layer_0,), kwargs = {})\n",
      "    %final_layer_2 : [#users=1] = call_module[target=final_layer.2](args = (%final_layer_1,), kwargs = {})\n",
      "    %final_layer_3 : [#users=1] = call_module[target=final_layer.3](args = (%final_layer_2,), kwargs = {})\n",
      "    %final_layer_4 : [#users=1] = call_module[target=final_layer.4](args = (%final_layer_3,), kwargs = {})\n",
      "    return [final_layer_4, input_1, fc_mu, fc_var]\n",
      "opcode         name           target                                                         args                                        kwargs\n",
      "-------------  -------------  -------------------------------------------------------------  ------------------------------------------  ----------------\n",
      "placeholder    input_1        input                                                          ()                                          {}\n",
      "placeholder    _kwargs        **kwargs                                                       ()                                          {}\n",
      "call_module    encoder_0_0    encoder.0.0                                                    (input_1,)                                  {}\n",
      "call_module    encoder_0_1    encoder.0.1                                                    (encoder_0_0,)                              {}\n",
      "call_module    encoder_0_2    encoder.0.2                                                    (encoder_0_1,)                              {}\n",
      "call_module    encoder_1_0    encoder.1.0                                                    (encoder_0_2,)                              {}\n",
      "call_module    encoder_1_1    encoder.1.1                                                    (encoder_1_0,)                              {}\n",
      "call_module    encoder_1_2    encoder.1.2                                                    (encoder_1_1,)                              {}\n",
      "call_module    encoder_2_0    encoder.2.0                                                    (encoder_1_2,)                              {}\n",
      "call_module    encoder_2_1    encoder.2.1                                                    (encoder_2_0,)                              {}\n",
      "call_module    encoder_2_2    encoder.2.2                                                    (encoder_2_1,)                              {}\n",
      "call_module    encoder_3_0    encoder.3.0                                                    (encoder_2_2,)                              {}\n",
      "call_module    encoder_3_1    encoder.3.1                                                    (encoder_3_0,)                              {}\n",
      "call_module    encoder_3_2    encoder.3.2                                                    (encoder_3_1,)                              {}\n",
      "call_module    encoder_4_0    encoder.4.0                                                    (encoder_3_2,)                              {}\n",
      "call_module    encoder_4_1    encoder.4.1                                                    (encoder_4_0,)                              {}\n",
      "call_module    encoder_4_2    encoder.4.2                                                    (encoder_4_1,)                              {}\n",
      "call_module    encoder_5_0    encoder.5.0                                                    (encoder_4_2,)                              {}\n",
      "call_module    encoder_5_1    encoder.5.1                                                    (encoder_5_0,)                              {}\n",
      "call_module    encoder_5_2    encoder.5.2                                                    (encoder_5_1,)                              {}\n",
      "call_function  flatten        <built-in method flatten of type object at 0x7f897973c420>     (encoder_5_2,)                              {'start_dim': 1}\n",
      "call_module    fc_mu          fc_mu                                                          (flatten,)                                  {}\n",
      "call_module    fc_var         fc_var                                                         (flatten,)                                  {}\n",
      "call_function  mul            <built-in function mul>                                        (0.5, fc_var)                               {}\n",
      "call_function  exp            <built-in method exp of type object at 0x7f897973c420>         (mul,)                                      {}\n",
      "call_function  randn_like     <built-in method randn_like of type object at 0x7f897973c420>  (exp,)                                      {}\n",
      "call_function  mul_1          <built-in function mul>                                        (randn_like, exp)                           {}\n",
      "call_function  add            <built-in function add>                                        (mul_1, fc_mu)                              {}\n",
      "call_module    decoder_input  decoder_input                                                  (add,)                                      {}\n",
      "call_method    view           view                                                           (decoder_input, -1, 1024, 1, 1)             {}\n",
      "call_module    decoder_0_0    decoder.0.0                                                    (view,)                                     {}\n",
      "call_module    decoder_0_1    decoder.0.1                                                    (decoder_0_0,)                              {}\n",
      "call_module    decoder_0_2    decoder.0.2                                                    (decoder_0_1,)                              {}\n",
      "call_module    decoder_1_0    decoder.1.0                                                    (decoder_0_2,)                              {}\n",
      "call_module    decoder_1_1    decoder.1.1                                                    (decoder_1_0,)                              {}\n",
      "call_module    decoder_1_2    decoder.1.2                                                    (decoder_1_1,)                              {}\n",
      "call_module    decoder_2_0    decoder.2.0                                                    (decoder_1_2,)                              {}\n",
      "call_module    decoder_2_1    decoder.2.1                                                    (decoder_2_0,)                              {}\n",
      "call_module    decoder_2_2    decoder.2.2                                                    (decoder_2_1,)                              {}\n",
      "call_module    decoder_3_0    decoder.3.0                                                    (decoder_2_2,)                              {}\n",
      "call_module    decoder_3_1    decoder.3.1                                                    (decoder_3_0,)                              {}\n",
      "call_module    decoder_3_2    decoder.3.2                                                    (decoder_3_1,)                              {}\n",
      "call_module    decoder_4_0    decoder.4.0                                                    (decoder_3_2,)                              {}\n",
      "call_module    decoder_4_1    decoder.4.1                                                    (decoder_4_0,)                              {}\n",
      "call_module    decoder_4_2    decoder.4.2                                                    (decoder_4_1,)                              {}\n",
      "call_module    final_layer_0  final_layer.0                                                  (decoder_4_2,)                              {}\n",
      "call_module    final_layer_1  final_layer.1                                                  (final_layer_0,)                            {}\n",
      "call_module    final_layer_2  final_layer.2                                                  (final_layer_1,)                            {}\n",
      "call_module    final_layer_3  final_layer.3                                                  (final_layer_2,)                            {}\n",
      "call_module    final_layer_4  final_layer.4                                                  (final_layer_3,)                            {}\n",
      "output         output         output                                                         ([final_layer_4, input_1, fc_mu, fc_var],)  {}\n"
     ]
    }
   ],
   "source": [
    "from vaes.vanilla_vae import VanillaVAE\n",
    "\n",
    "model = VanillaVAE(3, 64)\n",
    "symbolic_traced : torch.fx.GraphModule = torch.fx.symbolic_trace(model)\n",
    "print(symbolic_traced.graph)\n",
    "symbolic_traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'float'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'builtin_function_or_method'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.node.Node'> <class 'str'> <class 'str'>\n",
      "<class 'torch.fx.immutable_collections.immutable_list'> <class 'str'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for node in symbolic_traced.graph.nodes:\n",
    "    # print(node.op, node.is_impure())\n",
    "    # print(type(node.args))\n",
    "    # print(len(node.args))\n",
    "    if len(node.args) != 0:\n",
    "        print(type(node.args[0]), type(node.op), type(node.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_structure(model):\n",
    "    gm = torch.fx.symbolic_trace(model)\n",
    "    graph = gm.graph\n",
    "    node_list = graph.nodes\n",
    "    gm.graph.print_tabular()\n",
    "    tmp = dict()\n",
    "    for node in node_list:\n",
    "        print(node.name)\n",
    "        if node.op == 'call_module':\n",
    "            tmp[node.target] = []\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    tmp[node.target].extend([prev.target])\n",
    "                else:\n",
    "                    tmp[node.target].extend(tmp[prev.name])\n",
    "        else:\n",
    "            tmp[node.name] = []\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call module':\n",
    "                    tmp[node.name].extend(tmp[prev.target])\n",
    "                else:\n",
    "                    tmp[node.name].extend(tmp[prev.name])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_structure(model):\n",
    "    gm = torch.fx.symbolic_trace(model.cuda())\n",
    "    graph = gm.graph\n",
    "    node_list = graph.nodes\n",
    "    gm.graph.print_tabular()\n",
    "    tmp = dict()\n",
    "    for node in node_list:\n",
    "        print(node.name)\n",
    "        if node.op == 'call_module':\n",
    "            tmp[node.target] = set()\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    tmp[node.target].add(prev.target)\n",
    "                else:\n",
    "                    # tmp[node.target].extend(tmp[prev.name])\n",
    "                    tmp[node.target] |= tmp[prev.name]\n",
    "\n",
    "        else:\n",
    "            tmp[node.name] = set()\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    # tmp[node.name].extend(tmp[prev.target])\n",
    "                    tmp[node.name].add(prev.target)\n",
    "                else:\n",
    "                    # tmp[node.name].extend(tmp[prev.name])\n",
    "                    tmp[node.name] |= tmp[prev.name]\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name           target                                                         args                                        kwargs\n",
      "-------------  -------------  -------------------------------------------------------------  ------------------------------------------  ----------------\n",
      "placeholder    input_1        input                                                          ()                                          {}\n",
      "placeholder    _kwargs        **kwargs                                                       ()                                          {}\n",
      "call_module    encoder_0_0    encoder.0.0                                                    (input_1,)                                  {}\n",
      "call_module    encoder_0_1    encoder.0.1                                                    (encoder_0_0,)                              {}\n",
      "call_module    encoder_0_2    encoder.0.2                                                    (encoder_0_1,)                              {}\n",
      "call_module    encoder_1_0    encoder.1.0                                                    (encoder_0_2,)                              {}\n",
      "call_module    encoder_1_1    encoder.1.1                                                    (encoder_1_0,)                              {}\n",
      "call_module    encoder_1_2    encoder.1.2                                                    (encoder_1_1,)                              {}\n",
      "call_module    encoder_2_0    encoder.2.0                                                    (encoder_1_2,)                              {}\n",
      "call_module    encoder_2_1    encoder.2.1                                                    (encoder_2_0,)                              {}\n",
      "call_module    encoder_2_2    encoder.2.2                                                    (encoder_2_1,)                              {}\n",
      "call_module    encoder_3_0    encoder.3.0                                                    (encoder_2_2,)                              {}\n",
      "call_module    encoder_3_1    encoder.3.1                                                    (encoder_3_0,)                              {}\n",
      "call_module    encoder_3_2    encoder.3.2                                                    (encoder_3_1,)                              {}\n",
      "call_module    encoder_4_0    encoder.4.0                                                    (encoder_3_2,)                              {}\n",
      "call_module    encoder_4_1    encoder.4.1                                                    (encoder_4_0,)                              {}\n",
      "call_module    encoder_4_2    encoder.4.2                                                    (encoder_4_1,)                              {}\n",
      "call_module    encoder_5_0    encoder.5.0                                                    (encoder_4_2,)                              {}\n",
      "call_module    encoder_5_1    encoder.5.1                                                    (encoder_5_0,)                              {}\n",
      "call_module    encoder_5_2    encoder.5.2                                                    (encoder_5_1,)                              {}\n",
      "call_function  flatten        <built-in method flatten of type object at 0x7f897973c420>     (encoder_5_2,)                              {'start_dim': 1}\n",
      "call_module    fc_mu          fc_mu                                                          (flatten,)                                  {}\n",
      "call_module    fc_var         fc_var                                                         (flatten,)                                  {}\n",
      "call_function  mul            <built-in function mul>                                        (0.5, fc_var)                               {}\n",
      "call_function  exp            <built-in method exp of type object at 0x7f897973c420>         (mul,)                                      {}\n",
      "call_function  randn_like     <built-in method randn_like of type object at 0x7f897973c420>  (exp,)                                      {}\n",
      "call_function  mul_1          <built-in function mul>                                        (randn_like, exp)                           {}\n",
      "call_function  add            <built-in function add>                                        (mul_1, fc_mu)                              {}\n",
      "call_module    decoder_input  decoder_input                                                  (add,)                                      {}\n",
      "call_method    view           view                                                           (decoder_input, -1, 1024, 1, 1)             {}\n",
      "call_module    decoder_0_0    decoder.0.0                                                    (view,)                                     {}\n",
      "call_module    decoder_0_1    decoder.0.1                                                    (decoder_0_0,)                              {}\n",
      "call_module    decoder_0_2    decoder.0.2                                                    (decoder_0_1,)                              {}\n",
      "call_module    decoder_1_0    decoder.1.0                                                    (decoder_0_2,)                              {}\n",
      "call_module    decoder_1_1    decoder.1.1                                                    (decoder_1_0,)                              {}\n",
      "call_module    decoder_1_2    decoder.1.2                                                    (decoder_1_1,)                              {}\n",
      "call_module    decoder_2_0    decoder.2.0                                                    (decoder_1_2,)                              {}\n",
      "call_module    decoder_2_1    decoder.2.1                                                    (decoder_2_0,)                              {}\n",
      "call_module    decoder_2_2    decoder.2.2                                                    (decoder_2_1,)                              {}\n",
      "call_module    decoder_3_0    decoder.3.0                                                    (decoder_2_2,)                              {}\n",
      "call_module    decoder_3_1    decoder.3.1                                                    (decoder_3_0,)                              {}\n",
      "call_module    decoder_3_2    decoder.3.2                                                    (decoder_3_1,)                              {}\n",
      "call_module    decoder_4_0    decoder.4.0                                                    (decoder_3_2,)                              {}\n",
      "call_module    decoder_4_1    decoder.4.1                                                    (decoder_4_0,)                              {}\n",
      "call_module    decoder_4_2    decoder.4.2                                                    (decoder_4_1,)                              {}\n",
      "call_module    final_layer_0  final_layer.0                                                  (decoder_4_2,)                              {}\n",
      "call_module    final_layer_1  final_layer.1                                                  (final_layer_0,)                            {}\n",
      "call_module    final_layer_2  final_layer.2                                                  (final_layer_1,)                            {}\n",
      "call_module    final_layer_3  final_layer.3                                                  (final_layer_2,)                            {}\n",
      "call_module    final_layer_4  final_layer.4                                                  (final_layer_3,)                            {}\n",
      "output         output         output                                                         ([final_layer_4, input_1, fc_mu, fc_var],)  {}\n",
      "input_1\n",
      "_kwargs\n",
      "encoder_0_0\n",
      "encoder_0_1\n",
      "encoder_0_2\n",
      "encoder_1_0\n",
      "encoder_1_1\n",
      "encoder_1_2\n",
      "encoder_2_0\n",
      "encoder_2_1\n",
      "encoder_2_2\n",
      "encoder_3_0\n",
      "encoder_3_1\n",
      "encoder_3_2\n",
      "encoder_4_0\n",
      "encoder_4_1\n",
      "encoder_4_2\n",
      "encoder_5_0\n",
      "encoder_5_1\n",
      "encoder_5_2\n",
      "flatten\n",
      "fc_mu\n",
      "fc_var\n",
      "mul\n",
      "exp\n",
      "randn_like\n",
      "mul_1\n",
      "add\n",
      "decoder_input\n",
      "view\n",
      "decoder_0_0\n",
      "decoder_0_1\n",
      "decoder_0_2\n",
      "decoder_1_0\n",
      "decoder_1_1\n",
      "decoder_1_2\n",
      "decoder_2_0\n",
      "decoder_2_1\n",
      "decoder_2_2\n",
      "decoder_3_0\n",
      "decoder_3_1\n",
      "decoder_3_2\n",
      "decoder_4_0\n",
      "decoder_4_1\n",
      "decoder_4_2\n",
      "final_layer_0\n",
      "final_layer_1\n",
      "final_layer_2\n",
      "final_layer_3\n",
      "final_layer_4\n",
      "output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_1': set(),\n",
       " '_kwargs': set(),\n",
       " 'encoder.0.0': set(),\n",
       " 'encoder.0.1': {'encoder.0.0'},\n",
       " 'encoder.0.2': {'encoder.0.1'},\n",
       " 'encoder.1.0': {'encoder.0.2'},\n",
       " 'encoder.1.1': {'encoder.1.0'},\n",
       " 'encoder.1.2': {'encoder.1.1'},\n",
       " 'encoder.2.0': {'encoder.1.2'},\n",
       " 'encoder.2.1': {'encoder.2.0'},\n",
       " 'encoder.2.2': {'encoder.2.1'},\n",
       " 'encoder.3.0': {'encoder.2.2'},\n",
       " 'encoder.3.1': {'encoder.3.0'},\n",
       " 'encoder.3.2': {'encoder.3.1'},\n",
       " 'encoder.4.0': {'encoder.3.2'},\n",
       " 'encoder.4.1': {'encoder.4.0'},\n",
       " 'encoder.4.2': {'encoder.4.1'},\n",
       " 'encoder.5.0': {'encoder.4.2'},\n",
       " 'encoder.5.1': {'encoder.5.0'},\n",
       " 'encoder.5.2': {'encoder.5.1'},\n",
       " 'flatten': {'encoder.5.2'},\n",
       " 'fc_mu': {'encoder.5.2'},\n",
       " 'fc_var': {'encoder.5.2'},\n",
       " 'mul': {'fc_var'},\n",
       " 'exp': {'fc_var'},\n",
       " 'randn_like': {'fc_var'},\n",
       " 'mul_1': {'fc_var'},\n",
       " 'add': {'fc_mu', 'fc_var'},\n",
       " 'decoder_input': {'fc_mu', 'fc_var'},\n",
       " 'view': {'decoder_input'},\n",
       " 'decoder.0.0': {'decoder_input'},\n",
       " 'decoder.0.1': {'decoder.0.0'},\n",
       " 'decoder.0.2': {'decoder.0.1'},\n",
       " 'decoder.1.0': {'decoder.0.2'},\n",
       " 'decoder.1.1': {'decoder.1.0'},\n",
       " 'decoder.1.2': {'decoder.1.1'},\n",
       " 'decoder.2.0': {'decoder.1.2'},\n",
       " 'decoder.2.1': {'decoder.2.0'},\n",
       " 'decoder.2.2': {'decoder.2.1'},\n",
       " 'decoder.3.0': {'decoder.2.2'},\n",
       " 'decoder.3.1': {'decoder.3.0'},\n",
       " 'decoder.3.2': {'decoder.3.1'},\n",
       " 'decoder.4.0': {'decoder.3.2'},\n",
       " 'decoder.4.1': {'decoder.4.0'},\n",
       " 'decoder.4.2': {'decoder.4.1'},\n",
       " 'final_layer.0': {'decoder.4.2'},\n",
       " 'final_layer.1': {'final_layer.0'},\n",
       " 'final_layer.2': {'final_layer.1'},\n",
       " 'final_layer.3': {'final_layer.2'},\n",
       " 'final_layer.4': {'final_layer.3'},\n",
       " 'output': {'fc_mu', 'fc_var', 'final_layer.4'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_structure(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
