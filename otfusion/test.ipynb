{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ot\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def isnan(x):\n",
    "    return x != x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "class GroundMetric:\n",
    "    \"\"\"\n",
    "        Ground Metric object for Wasserstein computations:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, not_squared = False):\n",
    "        self.params = params\n",
    "        self.ground_metric_type = params.ground_metric\n",
    "        self.ground_metric_normalize = params.ground_metric_normalize\n",
    "        self.reg = params.reg\n",
    "        if hasattr(params, 'not_squared'):\n",
    "            self.squared = not params.not_squared\n",
    "        else:\n",
    "            # so by default squared will be on!\n",
    "            self.squared = not not_squared\n",
    "        self.mem_eff = params.ground_metric_eff\n",
    "\n",
    "    def _clip(self, ground_metric_matrix):\n",
    "        if self.params.debug:\n",
    "            print(\"before clipping\", ground_metric_matrix.data)\n",
    "\n",
    "        percent_clipped = (float((ground_metric_matrix >= self.reg * self.params.clip_max).long().sum().data) \\\n",
    "                           / ground_metric_matrix.numel()) * 100\n",
    "        print(\"percent_clipped is (assumes clip_min = 0) \", percent_clipped)\n",
    "        setattr(self.params, 'percent_clipped', percent_clipped)\n",
    "        # will keep the M' = M/reg in range clip_min and clip_max\n",
    "        ground_metric_matrix.clamp_(min=self.reg * self.params.clip_min,\n",
    "                                             max=self.reg * self.params.clip_max)\n",
    "        if self.params.debug:\n",
    "            print(\"after clipping\", ground_metric_matrix.data)\n",
    "        return ground_metric_matrix\n",
    "\n",
    "    def _normalize(self, ground_metric_matrix):\n",
    "\n",
    "        if self.ground_metric_normalize == \"log\":\n",
    "            ground_metric_matrix = torch.log1p(ground_metric_matrix)\n",
    "        elif self.ground_metric_normalize == \"max\":\n",
    "            print(\"Normalizing by max of ground metric and which is \", ground_metric_matrix.max())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.max()\n",
    "        elif self.ground_metric_normalize == \"median\":\n",
    "            print(\"Normalizing by median of ground metric and which is \", ground_metric_matrix.median())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.median()\n",
    "        elif self.ground_metric_normalize == \"mean\":\n",
    "            print(\"Normalizing by mean of ground metric and which is \", ground_metric_matrix.mean())\n",
    "            ground_metric_matrix = ground_metric_matrix / ground_metric_matrix.mean()\n",
    "        elif self.ground_metric_normalize == \"none\":\n",
    "            return ground_metric_matrix\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return ground_metric_matrix\n",
    "\n",
    "    def _sanity_check(self, ground_metric_matrix):\n",
    "        assert not (ground_metric_matrix < 0).any()\n",
    "        assert not (isnan(ground_metric_matrix).any())\n",
    "\n",
    "    def _cost_matrix_xy(self, x, y, p=2, squared = True):\n",
    "        # TODO: Use this to guarantee reproducibility of previous results and then move onto better way\n",
    "        \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "        # (n_l,1,n_{l-1})\n",
    "        x_col = x.unsqueeze(1)\n",
    "        # (1, m_l, m_{l-1})\n",
    "        # math induction -> n_{l-1} == m_{l-1}\n",
    "        y_lin = y.unsqueeze(0)\n",
    "        # c => (n_l, m_l, m_{l-1})\n",
    "        c = torch.sum((torch.abs(x_col - y_lin)) ** p, 2)\n",
    "        if not squared:\n",
    "            print(\"dont leave off the squaring of the ground metric\")\n",
    "            c = c ** (1/2)\n",
    "        # print(c.size())\n",
    "        if self.params.dist_normalize:\n",
    "            assert NotImplementedError\n",
    "        return c\n",
    "\n",
    "\n",
    "    def _pairwise_distances(self, x, y=None, squared=True):\n",
    "        '''\n",
    "        Source: https://discuss.pytorch.org/t/efficient-distance-matrix-computation/9065/2\n",
    "        Input: x is a Nxd matrix\n",
    "               y is an optional Mxd matirx\n",
    "        Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "                if y is not given then use 'y=x'.\n",
    "        i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "        '''\n",
    "        x_norm = (x ** 2).sum(1).view(-1, 1)\n",
    "        if y is not None:\n",
    "            y_t = torch.transpose(y, 0, 1)\n",
    "            y_norm = (y ** 2).sum(1).view(1, -1)\n",
    "        else:\n",
    "            y_t = torch.transpose(x, 0, 1)\n",
    "            y_norm = x_norm.view(1, -1)\n",
    "\n",
    "        dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "        # Ensure diagonal is zero if x=y\n",
    "        dist = torch.clamp(dist, min=0.0)\n",
    "\n",
    "        if self.params.activation_histograms and self.params.dist_normalize:\n",
    "            dist = dist/self.params.act_num_samples\n",
    "            print(\"Divide squared distances by the num samples\")\n",
    "\n",
    "        if not squared:\n",
    "            print(\"dont leave off the squaring of the ground metric\")\n",
    "            dist = dist ** (1/2)\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def _get_euclidean(self, coordinates, other_coordinates=None):\n",
    "        # TODO: Replace by torch.pdist (which is said to be much more memory efficient)\n",
    "\n",
    "        if other_coordinates is None:\n",
    "            matrix = torch.norm(\n",
    "                coordinates.view(coordinates.shape[0], 1, coordinates.shape[1]) \\\n",
    "                - coordinates, p=2, dim=2\n",
    "            )\n",
    "        else:\n",
    "            # memory efficient version\n",
    "            if self.mem_eff:\n",
    "                matrix = self._pairwise_distances(coordinates, other_coordinates, squared=self.squared)\n",
    "            else:\n",
    "                matrix = self._cost_matrix_xy(coordinates, other_coordinates, squared = self.squared)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _normed_vecs(self, vecs, eps=1e-9):\n",
    "        norms = torch.norm(vecs, dim=-1, keepdim=True)\n",
    "        print(\"stats of vecs are: mean {}, min {}, max {}, std {}\".format(\n",
    "            norms.mean(), norms.min(), norms.max(), norms.std()\n",
    "        ))\n",
    "        return vecs / (norms + eps)\n",
    "\n",
    "    def _get_cosine(self, coordinates, other_coordinates=None):\n",
    "        if other_coordinates is None:\n",
    "            matrix = coordinates / torch.norm(coordinates, dim=1, keepdim=True)\n",
    "            matrix = 1 - matrix @ matrix.t()\n",
    "        else:\n",
    "            matrix = 1 - torch.div(\n",
    "                coordinates @ other_coordinates.t(),\n",
    "                torch.norm(coordinates, dim=1).view(-1, 1) @ torch.norm(other_coordinates, dim=1).view(1, -1)\n",
    "            )\n",
    "        return matrix.clamp_(min=0)\n",
    "\n",
    "    def _get_angular(self, coordinates, other_coordinates=None):\n",
    "        pass\n",
    "\n",
    "    def get_metric(self, coordinates, other_coordinates=None):\n",
    "        get_metric_map = {\n",
    "            'euclidean': self._get_euclidean,\n",
    "            'cosine': self._get_cosine,\n",
    "            'angular': self._get_angular,\n",
    "        }\n",
    "        return get_metric_map[self.ground_metric_type](coordinates, other_coordinates)\n",
    "\n",
    "    def process(self, coordinates, other_coordinates=None):\n",
    "        print('Processing the coordinates to form ground_metric')\n",
    "        if self.params.geom_ensemble_type == 'wts' and self.params.normalize_wts:\n",
    "            print(\"In weight mode: normalizing weights to unit norm\")\n",
    "            coordinates = self._normed_vecs(coordinates)\n",
    "            if other_coordinates is not None:\n",
    "                other_coordinates = self._normed_vecs(other_coordinates)\n",
    "\n",
    "        ground_metric_matrix = self.get_metric(coordinates, other_coordinates)\n",
    "\n",
    "        if self.params.debug:\n",
    "            print(\"coordinates is \", coordinates)\n",
    "            if other_coordinates is not None:\n",
    "                print(\"other_coordinates is \", other_coordinates)\n",
    "            print(\"ground_metric_matrix is \", ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        ground_metric_matrix = self._normalize(ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        if self.params.clip_gm:\n",
    "            ground_metric_matrix = self._clip(ground_metric_matrix)\n",
    "\n",
    "        self._sanity_check(ground_metric_matrix)\n",
    "\n",
    "        if self.params.debug:\n",
    "            print(\"ground_metric_matrix at the end is \", ground_metric_matrix)\n",
    "\n",
    "        return ground_metric_matrix\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def cost_matrix(x, y, p=2):\n",
    "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "    x_col = x.unsqueeze(1)\n",
    "    y_lin = y.unsqueeze(0)\n",
    "    c = torch.sum((torch.abs(x_col - y_lin)) ** p, 2)\n",
    "    return c\n",
    "\n",
    "def get_histogram(args, idx, cardinality, layer_name, activations=None, return_numpy = True, float64=False):\n",
    "    if activations is None:\n",
    "        # returns a uniform measure\n",
    "        if not args.unbalanced:\n",
    "            print(\"returns a uniform measure of cardinality: \", cardinality)\n",
    "            return np.ones(cardinality)/cardinality\n",
    "        else:\n",
    "            return np.ones(cardinality)\n",
    "    else:\n",
    "        # return softmax over the activations raised to a temperature\n",
    "        # layer_name is like 'fc1.weight', while activations only contains 'fc1'\n",
    "        print(activations[idx].keys())\n",
    "        unnormalized_weights = activations[idx][layer_name.split('.')[0]]\n",
    "        print(\"For layer {},  shape of unnormalized weights is \".format(layer_name), unnormalized_weights.shape)\n",
    "        unnormalized_weights = unnormalized_weights.squeeze()\n",
    "        assert unnormalized_weights.shape[0] == cardinality\n",
    "\n",
    "        if return_numpy:\n",
    "            if float64:\n",
    "                return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0).data.cpu().numpy().astype(\n",
    "                    np.float64)\n",
    "            else:\n",
    "                return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0).data.cpu().numpy()\n",
    "        else:\n",
    "            return torch.softmax(unnormalized_weights / args.softmax_temperature, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def get_wassersteinized_layers_modularized(args, networks, activations=None, eps=1e-7, test_loader=None):\n",
    "    '''\n",
    "    Two neural networks that have to be averaged in geometric manner (i.e. layerwise).\n",
    "\n",
    "    The 1st network is aligned with respect to the other via wasserstein distance.\n",
    "    Also this assumes that all the layers are either fully connected or convolutional *(with no bias)*\n",
    "\n",
    "    :param networks: list of networks\n",
    "    :param activations: If not None, use it to build the activation histograms.\n",
    "    Otherwise assumes uniform distribution over neurons in a layer.\n",
    "    :return: list of layer weights 'wassersteinized'\n",
    "    '''\n",
    "\n",
    "    # simple_model_0, simple_model_1 = networks[0], networks[1]\n",
    "    # simple_model_0 = get_trained_model(0, model='simplenet')\n",
    "    # simple_model_1 = get_trained_model(1, model='simplenet')\n",
    "\n",
    "    avg_aligned_layers = []\n",
    "    # cumulative_T_var = None\n",
    "    T_var = None\n",
    "    # print(list(networks[0].parameters()))\n",
    "    previous_layer_shape = None\n",
    "    ground_metric_object = GroundMetric(args)\n",
    "\n",
    "    if args.eval_aligned:\n",
    "        model0_aligned_layers = []\n",
    "\n",
    "    if args.gpu_id==-1:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "\n",
    "    num_layers = len(list(zip(networks[0].parameters(), networks[1].parameters())))\n",
    "    # named_parameters is an generator\n",
    "    # for linear layer with bias, the affine matrix and bias will be two separate terms in named_parameters\n",
    "    for idx, ((layer0_name, fc_layer0_weight), (layer1_name, fc_layer1_weight)) in \\\n",
    "            enumerate(zip(networks[0].named_parameters(), networks[1].named_parameters())):\n",
    "\n",
    "        assert fc_layer0_weight.shape == fc_layer1_weight.shape\n",
    "        print(\"Previous layer shape is \", previous_layer_shape)\n",
    "        previous_layer_shape = fc_layer1_weight.shape\n",
    "\n",
    "        mu_cardinality = fc_layer0_weight.shape[0]\n",
    "        nu_cardinality = fc_layer1_weight.shape[0]\n",
    "\n",
    "        # mu = np.ones(fc_layer0_weight.shape[0])/fc_layer0_weight.shape[0]\n",
    "        # nu = np.ones(fc_layer1_weight.shape[0])/fc_layer1_weight.shape[0]\n",
    "\n",
    "        layer_shape = fc_layer0_weight.shape\n",
    "        if len(layer_shape) > 2:\n",
    "            is_conv = True\n",
    "            # For convolutional layers, it is (#out_channels, #in_channels, height, width)\n",
    "            fc_layer0_weight_data = fc_layer0_weight.data.view(fc_layer0_weight.shape[0], fc_layer0_weight.shape[1], -1)\n",
    "            fc_layer1_weight_data = fc_layer1_weight.data.view(fc_layer1_weight.shape[0], fc_layer1_weight.shape[1], -1)\n",
    "        else:\n",
    "            is_conv = False\n",
    "            fc_layer0_weight_data = fc_layer0_weight.data\n",
    "            fc_layer1_weight_data = fc_layer1_weight.data\n",
    "\n",
    "        if idx == 0:\n",
    "            if is_conv:\n",
    "                M = ground_metric_object.process(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n",
    "                                fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "                # M = cost_matrix(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n",
    "                #                 fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "            else:\n",
    "                # print(\"layer data is \", fc_layer0_weight_data, fc_layer1_weight_data)\n",
    "                M = ground_metric_object.process(fc_layer0_weight_data, fc_layer1_weight_data)\n",
    "                # M = cost_matrix(fc_layer0_weight, fc_layer1_weight)\n",
    "\n",
    "            aligned_wt = fc_layer0_weight_data\n",
    "        else:\n",
    "\n",
    "            print(\"shape of layer: model 0\", fc_layer0_weight_data.shape)\n",
    "            print(\"shape of layer: model 1\", fc_layer1_weight_data.shape)\n",
    "            print(\"shape of previous transport map\", T_var.shape)\n",
    "\n",
    "            # aligned_wt = None, this caches the tensor and causes OOM\n",
    "            if is_conv:\n",
    "                T_var_conv = T_var.unsqueeze(0).repeat(fc_layer0_weight_data.shape[2], 1, 1)\n",
    "                aligned_wt = torch.bmm(fc_layer0_weight_data.permute(2, 0, 1), T_var_conv).permute(1, 2, 0)\n",
    "\n",
    "                M = ground_metric_object.process(\n",
    "                    aligned_wt.contiguous().view(aligned_wt.shape[0], -1),\n",
    "                    fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1)\n",
    "                )\n",
    "            else:\n",
    "                # ? why \n",
    "                if fc_layer0_weight.data.shape[1] != T_var.shape[0]:\n",
    "                    # Handles the switch from convolutional layers to fc layers\n",
    "                    fc_layer0_unflattened = fc_layer0_weight.data.view(fc_layer0_weight.shape[0], T_var.shape[0], -1).permute(2, 0, 1)\n",
    "                    aligned_wt = torch.bmm(\n",
    "                        fc_layer0_unflattened,\n",
    "                        T_var.unsqueeze(0).repeat(fc_layer0_unflattened.shape[0], 1, 1)\n",
    "                    ).permute(1, 2, 0)\n",
    "                    aligned_wt = aligned_wt.contiguous().view(aligned_wt.shape[0], -1)\n",
    "                else:\n",
    "                    # print(\"layer data (aligned) is \", aligned_wt, fc_layer1_weight_data)\n",
    "                    aligned_wt = torch.matmul(fc_layer0_weight.data, T_var)\n",
    "                # M = cost_matrix(aligned_wt, fc_layer1_weight)\n",
    "                M = ground_metric_object.process(aligned_wt, fc_layer1_weight)\n",
    "            #     print(\"ground metric is \", M)\n",
    "            # if args.skip_last_layer and idx == (num_layers - 1):\n",
    "            #     print(\"Simple averaging of last layer weights. NO transport map needs to be computed\")\n",
    "            #     if args.ensemble_step != 0.5:\n",
    "            #         avg_aligned_layers.append((1 - args.ensemble_step) * aligned_wt +\n",
    "            #                               args.ensemble_step * fc_layer1_weight)\n",
    "            #     else:\n",
    "            #         avg_aligned_layers.append((aligned_wt + fc_layer1_weight)/2)\n",
    "            #     return avg_aligned_layers\n",
    "\n",
    "        if args.importance is None or (idx == num_layers -1):\n",
    "            mu = get_histogram(args, 0, mu_cardinality, layer0_name)\n",
    "            nu = get_histogram(args, 1, nu_cardinality, layer1_name)\n",
    "        # else:\n",
    "        #     # mu = _get_neuron_importance_histogram(args, aligned_wt, is_conv)\n",
    "        #     mu = _get_neuron_importance_histogram(args, fc_layer0_weight_data, is_conv)\n",
    "        #     nu = _get_neuron_importance_histogram(args, fc_layer1_weight_data, is_conv)\n",
    "        #     print(mu, nu)\n",
    "        #     assert args.proper_marginals\n",
    "\n",
    "        cpuM = M.data.cpu().numpy()\n",
    "        if args.exact:\n",
    "            T = ot.emd(mu, nu, cpuM)\n",
    "        else:\n",
    "            T = ot.bregman.sinkhorn(mu, nu, cpuM, reg=args.reg)\n",
    "        # T = ot.emd(mu, nu, log_cpuM)\n",
    "\n",
    "        if args.gpu_id!=-1:\n",
    "            T_var = torch.from_numpy(T).cuda(args.gpu_id).float()\n",
    "        else:\n",
    "            T_var = torch.from_numpy(T).float()\n",
    "\n",
    "        # torch.set_printoptions(profile=\"full\")\n",
    "        print(\"the transport map is \", T_var)\n",
    "        # torch.set_printoptions(profile=\"default\")\n",
    "\n",
    "        if args.correction:\n",
    "            if not args.proper_marginals:\n",
    "                # think of it as m x 1, scaling weights for m linear combinations of points in X\n",
    "                if args.gpu_id != -1:\n",
    "                    # marginals = torch.mv(T_var.t(), torch.ones(T_var.shape[0]).cuda(args.gpu_id))  # T.t().shape[1] = T.shape[0]\n",
    "                    marginals = torch.ones(T_var.shape[0]).cuda(args.gpu_id) / T_var.shape[0]\n",
    "                else:\n",
    "                    # marginals = torch.mv(T_var.t(),\n",
    "                    #                      torch.ones(T_var.shape[0]))  # T.t().shape[1] = T.shape[0]\n",
    "                    marginals = torch.ones(T_var.shape[0]) / T_var.shape[0]\n",
    "                marginals = torch.diag(1.0/(marginals + eps))  # take inverse\n",
    "                T_var = torch.matmul(T_var, marginals)\n",
    "            else:\n",
    "                # marginals_alpha = T_var @ torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)\n",
    "                marginals_beta = T_var.t() @ torch.ones(T_var.shape[0], dtype=T_var.dtype).to(device)\n",
    "\n",
    "                marginals = (1 / (marginals_beta + eps))\n",
    "                print(\"shape of inverse marginals beta is \", marginals_beta.shape)\n",
    "                print(\"inverse marginals beta is \", marginals_beta)\n",
    "\n",
    "                T_var = T_var * marginals\n",
    "                # i.e., how a neuron of 2nd model is constituted by the neurons of 1st model\n",
    "                # this should all be ones, and number equal to number of neurons in 2nd model\n",
    "                print(T_var.sum(dim=0))\n",
    "                # assert (T_var.sum(dim=0) == torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)).all()\n",
    "\n",
    "        if args.debug:\n",
    "            if idx == (num_layers - 1):\n",
    "                print(\"there goes the last transport map: \\n \", T_var)\n",
    "            else:\n",
    "                print(\"there goes the transport map at layer {}: \\n \".format(idx), T_var)\n",
    "\n",
    "            print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n",
    "\n",
    "        print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n",
    "        print(\"Here, trace is {} and matrix sum is {} \".format(torch.trace(T_var), torch.sum(T_var)))\n",
    "        setattr(args, 'trace_sum_ratio_{}'.format(layer0_name), (torch.trace(T_var) / torch.sum(T_var)).item())\n",
    "\n",
    "        if args.past_correction:\n",
    "            print(\"this is past correction for weight mode\")\n",
    "            print(\"Shape of aligned wt is \", aligned_wt.shape)\n",
    "            print(\"Shape of fc_layer0_weight_data is \", fc_layer0_weight_data.shape)\n",
    "            t_fc0_model = torch.matmul(T_var.t(), aligned_wt.contiguous().view(aligned_wt.shape[0], -1))\n",
    "        else:\n",
    "            t_fc0_model = torch.matmul(T_var.t(), fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1))\n",
    "\n",
    "        # Average the weights of aligned first layers\n",
    "        if args.ensemble_step != 0.5:\n",
    "            geometric_fc = ((1-args.ensemble_step) * t_fc0_model +\n",
    "                            args.ensemble_step * fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n",
    "        else:\n",
    "            geometric_fc = (t_fc0_model + fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))/2\n",
    "        if is_conv and layer_shape != geometric_fc.shape:\n",
    "            geometric_fc = geometric_fc.view(layer_shape)\n",
    "        avg_aligned_layers.append(geometric_fc)\n",
    "\n",
    "        # # get the performance of the model 0 aligned with respect to the model 1\n",
    "        # if args.eval_aligned:\n",
    "        #     if is_conv and layer_shape != t_fc0_model.shape:\n",
    "        #         t_fc0_model = t_fc0_model.view(layer_shape)\n",
    "        #     model0_aligned_layers.append(t_fc0_model)\n",
    "        #     _, acc = update_model(args, networks[0], model0_aligned_layers, test=True,\n",
    "        #                           test_loader=test_loader, idx=0)\n",
    "        #     print(\"For layer idx {}, accuracy of the updated model is {}\".format(idx, acc))\n",
    "        #     setattr(args, 'model0_aligned_acc_layer_{}'.format(str(idx)), acc)\n",
    "        #     if idx == (num_layers - 1):\n",
    "        #         setattr(args, 'model0_aligned_acc', acc)\n",
    "\n",
    "    return avg_aligned_layers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        # print(\"After encoder: \", result.shape)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(\"After encoder and flatten: \", result.shape)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        # print(\"mu:\", mu.shape)\n",
    "        # print(\"log_var;\", log_var.shape)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        # print(\"after decoder_input layer:\", result.shape)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        # print(\"after reshape:\", result.shape)\n",
    "        result = self.decoder(result)\n",
    "        # print(\"after decoder:\", result.shape)\n",
    "        result = self.final_layer(result)\n",
    "        # print(\"after final layer:\", result.shape)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "model1 = VanillaVAE(3, 128)\n",
    "model2 = VanillaVAE(3, 128)\n",
    "dict1 = dict(torch.load('vae_nob_bias_adagrad_1.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "dict2 = dict(torch.load('vae_nob_bias_adagrad_2.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "tmp1 = dict()\n",
    "tmp2 = dict()\n",
    "\n",
    "# print(dict1.keys())\n",
    "for k in dict1.keys():\n",
    "    v1 = dict1[k]\n",
    "    v2 = dict2[k]\n",
    "    k = k[6:]\n",
    "    # print(k)\n",
    "    # if \"running\" in k:\n",
    "    #     print(v1 - v2)\n",
    "    tmp1[k] = v1\n",
    "    tmp2[k] = v2\n",
    "\n",
    "# for k in dict2.keys():\n",
    "#     v = dict2[k]\n",
    "#     k = k[6:]\n",
    "#     # print(k)\n",
    "#     tmp2[k] = v\n",
    "\n",
    "model1.load_state_dict(tmp1)\n",
    "model2.load_state_dict(tmp2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model1, model2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from parameters import get_parameters\n",
    "\n",
    "args = get_parameters()\n",
    "\n",
    "get_wassersteinized_layers_modularized(args, [model1, model2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import CelebA\n",
    "import torch.nn.functional as F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.CenterCrop(148),\n",
    "                                        transforms.Resize(64),\n",
    "                                        transforms.ToTensor(),])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "test_data = CelebA(root='/home/chenyuheng/celeba', split='test', download=False, transform=train_transforms) "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "def loaddata():\n",
    "    return torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)   \n",
    "\n",
    "from tqdm import tqdm\n",
    "def test(model):\n",
    "    model.cuda()\n",
    "    dataloader = loaddata()\n",
    "    i = 0\n",
    "    loss = 0.0\n",
    "    recon_loss = 0.0\n",
    "    kl_loss = 0.0\n",
    "    for img, _ in tqdm(dataloader):\n",
    "        # print(img)\n",
    "        out = model(img.cuda())\n",
    "        # loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (i + 1)\n",
    "        loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (i + 1)\n",
    "        recon_loss = recon_loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['Reconstruction_Loss'].item() / (i + 1)\n",
    "        kl_loss = kl_loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['KLD'].item() / (i + 1)\n",
    "        # loss = loss * (i / (i + 1)) + model.loss_function(*out, M_N=0.00025)['loss'].item() / (64 * (i + 1))\n",
    "        i += 1\n",
    "    return {'loss': loss, 'recon_loss': recon_loss, 'kl_loss': kl_loss}\n",
    "\n",
    "def get_avg_model(t):\n",
    "    avg_model = VanillaVAE(3, 128)\n",
    "    avg_tmp = dict()\n",
    "    for k in dict1.keys():\n",
    "        v1 = dict1[k]\n",
    "        v2 = dict2[k]\n",
    "        k = k[6:]\n",
    "        avg_tmp[k] = t * v1 + (1 - t) * v2\n",
    "    avg_model.load_state_dict(avg_tmp)\n",
    "    return avg_model\n",
    "\n",
    "for t in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    print(test(get_avg_model(t)))\n",
    "# print(test(fused_model))\n",
    "print(test(model1), test(model2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "test(model1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "test(model2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "print(model1, model2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "param_diff = 0.0\n",
    "for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "    print(param1.shape, param2.shape)\n",
    "    param_diff += torch.square(param1.cpu() - param2.cpu()).sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "param_diff"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "source": [
    "model3 = VanillaVAE(3, 128)\n",
    "model4 = VanillaVAE(3, 128)\n",
    "\n",
    "dict3 = dict(torch.load('last7.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "dict4 = dict(torch.load('last8.ckpt', map_location=torch.device('cpu'))['state_dict'])\n",
    "tmp3 = dict()\n",
    "tmp4 = dict()\n",
    "\n",
    "# print(dict1.keys())\n",
    "for k in dict3.keys():\n",
    "    v1 = dict3[k]\n",
    "    v2 = dict4[k]\n",
    "    k = k[6:]\n",
    "    # print(k)\n",
    "    # if \"running\" in k:\n",
    "    #     print(v1 - v2)\n",
    "    tmp3[k] = v1\n",
    "    tmp4[k] = v2\n",
    "\n",
    "# for k in dict2.keys():\n",
    "#     v = dict2[k]\n",
    "#     k = k[6:]\n",
    "#     # print(k)\n",
    "#     tmp2[k] = v\n",
    "\n",
    "model3.load_state_dict(tmp3)\n",
    "model4.load_state_dict(tmp4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "param_diff = 0.0\n",
    "for param1, param2 in zip(model3.parameters(), model4.parameters()):\n",
    "    # print(param1.shape, param2.shape)\n",
    "    param_diff += torch.square(param1.cpu() - param2.cpu()).sum()\n",
    "param_diff"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "lst1 = [1, 2, 3]\n",
    "lst2 = [4, 5, 6]\n",
    "for i,j in zip(lst1, lst2):\n",
    "    print(i, j)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "import PIL\n",
    "# import PIL.Image\n",
    "img = PIL.Image.open(\"origin_VanillaVAE_Epoch_0.png\")\n",
    "train_transforms(img).shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model1(train_transforms(img).unsqueeze(0))\n",
    "# model1(train_transforms(img).unsqueeze(0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "t = Tensor(1, 3, 64, 64)\n",
    "t"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from vaes.vanilla_vae import VanillaVAE\n",
    "\n",
    "model = VanillaVAE(3, 64)\n",
    "symbolic_traced : torch.fx.GraphModule = torch.fx.symbolic_trace(model)\n",
    "print(symbolic_traced.graph)\n",
    "symbolic_traced.graph.print_tabular()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "for node in symbolic_traced.graph.nodes:\n",
    "    # print(node.op, node.is_impure())\n",
    "    # print(type(node.args))\n",
    "    # print(len(node.args))\n",
    "    if len(node.args) != 0:\n",
    "        print(type(node.args[0]), type(node.op), type(node.target))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "def get_model_structure(model):\n",
    "    gm = torch.fx.symbolic_trace(model)\n",
    "    graph = gm.graph\n",
    "    node_list = graph.nodes\n",
    "    gm.graph.print_tabular()\n",
    "    tmp = dict()\n",
    "    for node in node_list:\n",
    "        print(node.name)\n",
    "        if node.op == 'call_module':\n",
    "            tmp[node.target] = []\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    tmp[node.target].extend([prev.target])\n",
    "                else:\n",
    "                    tmp[node.target].extend(tmp[prev.name])\n",
    "        else:\n",
    "            tmp[node.name] = []\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call module':\n",
    "                    tmp[node.name].extend(tmp[prev.target])\n",
    "                else:\n",
    "                    tmp[node.name].extend(tmp[prev.name])\n",
    "    return tmp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "def get_model_structure(model):\n",
    "    gm = torch.fx.symbolic_trace(model.cuda())\n",
    "    graph = gm.graph\n",
    "    node_list = graph.nodes\n",
    "    gm.graph.print_tabular()\n",
    "    tmp = dict()\n",
    "    for node in node_list:\n",
    "        print(node.name)\n",
    "        if node.op == 'call_module':\n",
    "            tmp[node.target] = set()\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    tmp[node.target].add(prev.target)\n",
    "                else:\n",
    "                    # tmp[node.target].extend(tmp[prev.name])\n",
    "                    tmp[node.target] |= tmp[prev.name]\n",
    "\n",
    "        else:\n",
    "            tmp[node.name] = set()\n",
    "            for prev in node.all_input_nodes:\n",
    "                if prev.op == 'call_module':\n",
    "                    # tmp[node.name].extend(tmp[prev.target])\n",
    "                    tmp[node.name].add(prev.target)\n",
    "                else:\n",
    "                    # tmp[node.name].extend(tmp[prev.name])\n",
    "                    tmp[node.name] |= tmp[prev.name]\n",
    "\n",
    "    return tmp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "get_model_structure(model)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
